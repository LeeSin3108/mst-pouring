<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Attention Guided in Multimodal senses for robot learning pouring task">
  <meta name="keywords" content="Multimodal Sensing, Attention, Concurrent Multitask">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Attention Guided in Multimodal senses for robot learning pouring task</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Attention Guided in Multimodal senses for robot learning pouring task</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="null">TBD</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="null">TBD</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="null">TBD</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>TBD</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero human_demo">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="human_demo" controls preload="metadata" autoplay loop playsinline height="100%">
        <source src="./static/videos/multisense_human_demo.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Human use divided attention to concurrently perform multiple tasks and selected attention to overcome disturbances
         based on multimodal sensory inputs to accomplish complex tasks stably and efficiently.
      </h2>
    </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <img src="./static/images/MST-Pouring-struct.png"
              class="interpolation-image"
              alt="Interpolate start reference image."/>
        <p class="columns is-centered">Attention guided multimodal sense diffusion strategy framework</p>
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Different Init Pouring State</h2>
          <div class="column is-full-width">
            <p>
              Strategy with Multimodal Sense Transformer (MST) is able to perceive the 
              completion of the pouring action through  both visual and auditory perception
              at different initial water levels, thus ending the pouring action.
            </p>
          </div>
          <p>The initial water volume is empty </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="90%">
            <source src="./static/videos/no_disturb_0-cutte.mp4"
                    type="video/mp4">
          </video>
          <p>Initial small amount of water (about 50ml) </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="90%">
            <source src="./static/videos/no_disturb_1-cutte.mp4"
                    type="video/mp4">
          </video>
          <p>Initial medium amount of water (about 150ml) </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="90%">
            <source src="./static/videos/no_disturb_2-cutte.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Short term visual occlusion</h2>
        <div class="columns is-centered">
          <div class="column content">
            <div class="column is-full-width">
              <p>
                MST according to auditory perception based on attention, 
                weaken the impact of visual perception on strategy during occlusion, 
                and continue to use visual after restoring vision to end the pouring action
              </p>
            </div>
            <p>Vision Only </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/short_vcut_VO-cutte.mp4"
                      type="video/mp4">
            </video>
            <p>Vision Audio Concatenate </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/short_vcut_VAC-cutte.mp4"
                      type="video/mp4">
            </video>
            <p>MST (Ours) </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/short_vcut_MST-cutte.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-3">Long term visual occlusion</h2>
        <div class="columns is-centered">
          <div class="column content">
            <div class="column is-full-width">
              <p>
                By amplifying auditory perception through attention and 
                reducing the impact of visual perception on the strategy 
                during occlusion, MST can continue pouring water 
                and stably complete the end pouring action relying on sound.
              </p>
            </div>
            <p>Vision Only </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/long_vcut_VO-cutte.mp4"
                      type="video/mp4">
            </video>
            <p>Vision Audio Concatenate </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/long_vcut_VAC-cutte.mp4"
                      type="video/mp4">
            </video>
            <p>MST (Ours) </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/long_vcut_MST-cutte.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Continuously noise</h2>
        <div class="columns is-centered">
          <div class="column content">
            <div class="column is-full-width">
              <p>
                Attention is dominated by visual perception, 
                and in sound perception, it is best to focus on 
                the parts related to water sound, so as to continue 
                pouring water and complete the pouring action stably.
              </p>
            </div>
            <p>Vision Audio Concatenate in opera noise  </p>
            <video id="matting-video" controls playsinline height="120%">
              <source src="./static/videos/audio_opera_VAC.mp4"
                      type="video/mp4">
            </video>
            <p>MST (Ours) in opera noise </p>
            <video id="matting-video" controls playsinline height="120%">
              <source src="./static/videos/audio_opera_MST.mp4"
                      type="video/mp4">
            </video>
            <p>MST (Ours) in restaurant background noise </p>
            <video id="matting-video" controls playsinline height="120%">
              <source src="./static/videos/audio_restr_MST.mp4"
                      type="video/mp4">
            </video>
          </div>

          </div>
        </div>
      </div>
    </div>

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Attention Visualize</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
